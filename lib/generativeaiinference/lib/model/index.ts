/**
 * Generative AI Service Inference API
 * OCI Generative AI is a fully managed service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases for text generation, summarization, and text embeddings. 

Use the Generative AI service inference API to access your custom model endpoints, or to try the out-of-the-box models to [generate text](#/en/generative-ai-inference/latest/GenerateTextResult/GenerateText), [summarize](#/en/generative-ai-inference/latest/SummarizeTextResult/SummarizeText), and [create text embeddings](#/en/generative-ai-inference/latest/EmbedTextResult/EmbedText).

To use a Generative AI custom model for inference, you must first create an endpoint for that model. Use the [Generative AI service management API](/#/en/generative-ai/latest/) to [create a custom model](#/en/generative-ai/latest/Model/) by fine-tuning an out-of-the-box model, or a previous version of a custom model, using your own data. Fine-tune the custom model on a  [fine-tuning dedicated AI cluster](#/en/generative-ai/latest/DedicatedAiCluster/). Then, create a [hosting dedicated AI cluster](#/en/generative-ai/latest/DedicatedAiCluster/) with an [endpoint](#/en/generative-ai/latest/Endpoint/) to host your custom model. For resource management in the Generative AI service, use the [Generative AI service management API](/#/en/generative-ai/latest/).

To learn more about the service, see the [Generative AI documentation](/iaas/Content/generative-ai/home.htm).

 * OpenAPI spec version: 20231130
 * 
 *
 * NOTE: This class is auto generated by OracleSDKGenerator.
 * Do not edit the class manually.
 *
 * Copyright (c) 2020, 2024, Oracle and/or its affiliates.  All rights reserved.
 * This software is dual-licensed to you under the Universal Permissive License (UPL) 1.0 as shown at https://oss.oracle.com/licenses/upl or Apache License 2.0 as shown at http://www.apache.org/licenses/LICENSE-2.0. You may choose either license.
 */

import * as Choice from "./choice";
export import Choice = Choice.Choice;
import * as EmbedTextDetails from "./embed-text-details";
export import EmbedTextDetails = EmbedTextDetails.EmbedTextDetails;
import * as EmbedTextResult from "./embed-text-result";
export import EmbedTextResult = EmbedTextResult.EmbedTextResult;
import * as GenerateTextDetails from "./generate-text-details";
export import GenerateTextDetails = GenerateTextDetails.GenerateTextDetails;
import * as GenerateTextResult from "./generate-text-result";
export import GenerateTextResult = GenerateTextResult.GenerateTextResult;
import * as GeneratedText from "./generated-text";
export import GeneratedText = GeneratedText.GeneratedText;
import * as LlmInferenceRequest from "./llm-inference-request";
export import LlmInferenceRequest = LlmInferenceRequest.LlmInferenceRequest;
import * as LlmInferenceResponse from "./llm-inference-response";
export import LlmInferenceResponse = LlmInferenceResponse.LlmInferenceResponse;
import * as Logprobs from "./logprobs";
export import Logprobs = Logprobs.Logprobs;
import * as ServingMode from "./serving-mode";
export import ServingMode = ServingMode.ServingMode;
import * as SummarizeTextDetails from "./summarize-text-details";
export import SummarizeTextDetails = SummarizeTextDetails.SummarizeTextDetails;
import * as SummarizeTextResult from "./summarize-text-result";
export import SummarizeTextResult = SummarizeTextResult.SummarizeTextResult;
import * as TokenLikelihood from "./token-likelihood";
export import TokenLikelihood = TokenLikelihood.TokenLikelihood;

import * as CohereLlmInferenceRequest from "./cohere-llm-inference-request";
export import CohereLlmInferenceRequest = CohereLlmInferenceRequest.CohereLlmInferenceRequest;
import * as CohereLlmInferenceResponse from "./cohere-llm-inference-response";
export import CohereLlmInferenceResponse = CohereLlmInferenceResponse.CohereLlmInferenceResponse;
import * as DedicatedServingMode from "./dedicated-serving-mode";
export import DedicatedServingMode = DedicatedServingMode.DedicatedServingMode;
import * as LlamaLlmInferenceRequest from "./llama-llm-inference-request";
export import LlamaLlmInferenceRequest = LlamaLlmInferenceRequest.LlamaLlmInferenceRequest;
import * as LlamaLlmInferenceResponse from "./llama-llm-inference-response";
export import LlamaLlmInferenceResponse = LlamaLlmInferenceResponse.LlamaLlmInferenceResponse;
import * as OnDemandServingMode from "./on-demand-serving-mode";
export import OnDemandServingMode = OnDemandServingMode.OnDemandServingMode;
